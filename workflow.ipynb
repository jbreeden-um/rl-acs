{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaf5d2db-5453-469c-9357-c37dedea7e89",
   "metadata": {},
   "source": [
    "# Sample Workflow for d3rlpy Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bfb6706a-eaec-4154-be04-e2b7c168479a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import math\n",
    "import subprocess\n",
    "import os\n",
    "import d3rlpy\n",
    "plt.style.use('matplotlibrc')\n",
    "\n",
    "from Python.data_sampler import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361d8a2f-129e-4b9c-81cd-55314fabeabd",
   "metadata": {},
   "source": [
    "## Building an MDPDataset\n",
    "\n",
    "We first read in a large batch of samples from the file. As `d3rlpy` wants it in the form (observations, actions, rewards, terminal flags), we go ahead and do that. Here's a helper function to get a dataset from a list of chunks of your choosing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3456d718-b8bb-460f-a429-9d99633145f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(chunks : list, batch_size=30000, \n",
    "                path=\"collected_data/rl_det_small.txt\") -> d3rlpy.dataset.MDPDataset :\n",
    "    random.seed(0)\n",
    "    samples = DataSampler(path_to_data=path)\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    next_states = []\n",
    "    for chunk in chunks:\n",
    "        samples.use_chunk(chunk)\n",
    "        samples.read_chunk()\n",
    "        [statesChunk, actionsChunk, rewardsChunk, nextStatesChunk] = samples.get_batch(batch_size)\n",
    "        states.append(statesChunk)\n",
    "        actions.append(actionsChunk)\n",
    "        rewards.append(rewardsChunk)\n",
    "        next_states.append(nextStatesChunk)\n",
    "    states = torch.cat(states)\n",
    "    actions = torch.cat(actions)\n",
    "    rewards = torch.cat(rewards)\n",
    "    next_states = torch.cat(next_states)\n",
    "    terminals = np.zeros(len(states))\n",
    "    terminals[::100] = 1 #episode length 100, change if necessary\n",
    "    print(states.shape)\n",
    "    dataset = d3rlpy.dataset.MDPDataset(states.numpy(), \n",
    "                                        actions.numpy(), \n",
    "                                        rewards.numpy(), terminals)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaff27f-fd9a-4ad2-81c6-fca5f5fe6ce9",
   "metadata": {},
   "source": [
    "We can build the dataset from there, just like this, and split into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f543168b-2603-47ed-bff8-d0bafb40bb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00000000e+00  7.95731469e+08 -8.17891077e-02 -1.19999531e-03\n",
      "  7.39998658e-03  0.00000000e+00 -5.33423489e+00 -1.57091618e+00\n",
      "  2.09713430e-01 -2.63658359e-01  6.00000000e-01]\n",
      "Read chunk # 4 out of 10000\n",
      "[ 0.00000000e+00  7.95731469e+08  1.24610892e-01  2.40000469e-03\n",
      " -7.60001342e-03  0.00000000e+00 -5.33423489e+00 -1.57091618e+00\n",
      " -2.20016566e-01  3.79282423e-01 -6.00000000e-01]\n",
      "Read chunk # 6 out of 10000\n",
      "[ 0.00000000e+00  7.95731469e+08 -9.01891077e-02  1.08000047e-02\n",
      "  3.99986580e-04  0.00000000e+00 -5.33423489e+00 -1.57091618e+00\n",
      " -3.17973088e-02 -2.40776052e-01  6.00000000e-01]\n",
      "Read chunk # 8 out of 10000\n",
      "[ 0.00000000e+00  7.95731469e+08  6.91108923e-02 -5.99999531e-03\n",
      " -6.00001342e-03  0.00000000e+00 -5.33423489e+00 -1.57091618e+00\n",
      " -1.42355434e-01  2.22081792e-01 -6.00000000e-01]\n",
      "Read chunk # 10 out of 10000\n",
      "torch.Size([111080, 6])\n"
     ]
    }
   ],
   "source": [
    "dataset = get_dataset([3,5,7,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "8014463e-ab16-4498-ae33-63b91854534c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The behavior policy value statistics are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean': -4.1227446,\n",
       " 'std': 2.4676569,\n",
       " 'min': -12.578855,\n",
       " 'max': 0.0,\n",
       " 'histogram': (array([ 26,   9,   7,   7,   8,   7,  10,  13,  27,  54,  56,  73, 109,\n",
       "          84, 186, 148, 124,  83,  67,  13]),\n",
       "  array([-12.578855 , -11.949912 , -11.320969 , -10.692026 , -10.063084 ,\n",
       "          -9.434141 ,  -8.805199 ,  -8.176255 ,  -7.5473127,  -6.9183702,\n",
       "          -6.2894273,  -5.6604843,  -5.031542 ,  -4.4025993,  -3.7736564,\n",
       "          -3.1447136,  -2.515771 ,  -1.8868282,  -1.2578855,  -0.6289427,\n",
       "           0.       ], dtype=float32))}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The behavior policy value statistics are:\")\n",
    "dataset.compute_stats()['return']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f051bee1-1f8b-4acc-a658-d80a5b4df54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_episodes, test_episodes = train_test_split(dataset, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1025ba17-f95f-426b-adc1-9312e5e180fa",
   "metadata": {},
   "source": [
    "## Setting up an Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f5ef782b-6847-4fa4-8d95-3fa8904866bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from d3rlpy.algos import CQL\n",
    "model = CQL(q_func_factory='qr', #quantile regression q function, but you don't have to use this\n",
    "            use_gpu=False) #change it to true if you have one\n",
    "model.build_with_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2fd44e0e-0ab2-4ab0-8f96-7a8eb382cda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.037882303384637175\n"
     ]
    }
   ],
   "source": [
    "from d3rlpy.metrics.scorer import td_error_scorer\n",
    "from d3rlpy.metrics.scorer import average_value_estimation_scorer\n",
    "from d3rlpy.metrics.scorer import initial_state_value_estimation_scorer\n",
    "\n",
    "# calculate metrics with test dataset\n",
    "ave_error_init = average_value_estimation_scorer(model, test_episodes)\n",
    "print(ave_error_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9b0384d5-a843-4d30-a355-404215f27fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-06 18:44.47 [debug    ] RoundIterator is selected.\n",
      "2022-04-06 18:44.47 [info     ] Directory is created at d3rlpy_logs/CQL_20220406184447\n",
      "2022-04-06 18:44.47 [warning  ] Skip building models since they're already built.\n",
      "2022-04-06 18:44.47 [info     ] Parameters are saved to d3rlpy_logs/CQL_20220406184447/params.json params={'action_scaler': None, 'actor_encoder_factory': {'type': 'default', 'params': {'activation': 'relu', 'use_batch_norm': False, 'dropout_rate': None}}, 'actor_learning_rate': 0.0001, 'actor_optim_factory': {'optim_cls': 'Adam', 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False}, 'alpha_learning_rate': 0.0001, 'alpha_optim_factory': {'optim_cls': 'Adam', 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False}, 'alpha_threshold': 10.0, 'batch_size': 256, 'conservative_weight': 5.0, 'critic_encoder_factory': {'type': 'default', 'params': {'activation': 'relu', 'use_batch_norm': False, 'dropout_rate': None}}, 'critic_learning_rate': 0.0003, 'critic_optim_factory': {'optim_cls': 'Adam', 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False}, 'gamma': 0.99, 'generated_maxlen': 100000, 'initial_alpha': 1.0, 'initial_temperature': 1.0, 'n_action_samples': 10, 'n_critics': 2, 'n_frames': 1, 'n_steps': 1, 'q_func_factory': {'type': 'qr', 'params': {'bootstrap': False, 'share_encoder': False, 'n_quantiles': 32}}, 'real_ratio': 1.0, 'reward_scaler': None, 'scaler': None, 'soft_q_backup': False, 'target_reduction_type': 'min', 'tau': 0.005, 'temp_learning_rate': 0.0001, 'temp_optim_factory': {'optim_cls': 'Adam', 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False}, 'use_gpu': None, 'algorithm': 'CQL', 'observation_shape': (6,), 'action_size': 3}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6998af0cdac3408c8fc36ec1d21fe761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/5:   0%|          | 0/343 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-06 18:45.19 [info     ] CQL_20220406184447: epoch=1 step=343 epoch=1 metrics={'time_sample_batch': 0.00034665783362207886, 'time_algorithm_update': 0.09301192836928299, 'temp_loss': 4.434645806387632, 'temp': 0.9835434387446145, 'alpha_loss': -10.887004488411172, 'alpha': 1.0149100758243927, 'critic_loss': 11.504611487291298, 'actor_loss': -0.4734545949420216, 'time_step': 0.09343911398951583, 'td_error': 0.23003137005684537, 'init_value': 0.358516663312912, 'ave_value': 0.3585621578675918} step=343\n",
      "2022-04-06 18:45.19 [info     ] Model parameters are saved to d3rlpy_logs/CQL_20220406184447/model_343.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96012a67ea6047fbb2892a1cee44943b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/5:   0%|          | 0/343 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-06 18:45.51 [info     ] CQL_20220406184447: epoch=2 step=686 epoch=2 metrics={'time_sample_batch': 0.00030645267608909495, 'time_algorithm_update': 0.09020866944560504, 'temp_loss': 2.2606965619690564, 'temp': 0.9586732479643196, 'alpha_loss': 2.6586992984132585, 'alpha': 1.0215142763738383, 'critic_loss': -1.0089621576901429, 'actor_loss': 1.6619744485043229, 'time_step': 0.09059124368272788, 'td_error': 0.2244357153328237, 'init_value': 1.1041191816329956, 'ave_value': 1.1022191792748874} step=686\n",
      "2022-04-06 18:45.51 [info     ] Model parameters are saved to d3rlpy_logs/CQL_20220406184447/model_686.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb0b14030dae4b689951662ee3228f2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/5:   0%|          | 0/343 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-06 18:46.25 [info     ] CQL_20220406184447: epoch=3 step=1029 epoch=3 metrics={'time_sample_batch': 0.00031283228459928194, 'time_algorithm_update': 0.0978050558629606, 'temp_loss': 1.3170170980361737, 'temp': 0.9418875839202814, 'alpha_loss': 7.240036223442145, 'alpha': 0.9993701052387671, 'critic_loss': -4.852770173514897, 'actor_loss': 2.0584373998920005, 'time_step': 0.09819418626345977, 'td_error': 0.15648950364463302, 'init_value': 1.2321447134017944, 'ave_value': 1.228177076818056} step=1029\n",
      "2022-04-06 18:46.25 [info     ] Model parameters are saved to d3rlpy_logs/CQL_20220406184447/model_1029.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eb99420ce354845a23445bdfe328c2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/5:   0%|          | 0/343 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-06 18:46.58 [info     ] CQL_20220406184447: epoch=4 step=1372 epoch=4 metrics={'time_sample_batch': 0.0003068141269961877, 'time_algorithm_update': 0.09243900157272295, 'temp_loss': 0.7861016806291075, 'temp': 0.9292568977998228, 'alpha_loss': 9.583377957691605, 'alpha': 0.9631524921853758, 'critic_loss': -6.50581574926571, 'actor_loss': 2.4692244161322234, 'time_step': 0.09282089391880759, 'td_error': 0.11061606113975374, 'init_value': 1.2937169075012207, 'ave_value': 1.2894880254542218} step=1372\n",
      "2022-04-06 18:46.58 [info     ] Model parameters are saved to d3rlpy_logs/CQL_20220406184447/model_1372.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77d1f5601baa429a8e0d4d65a6dc3915",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/5:   0%|          | 0/343 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-06 18:47.32 [info     ] CQL_20220406184447: epoch=5 step=1715 epoch=5 metrics={'time_sample_batch': 0.0003132173688349154, 'time_algorithm_update': 0.09711221842307044, 'temp_loss': 0.4085727234102199, 'temp': 0.9204262795670735, 'alpha_loss': 10.972351727610775, 'alpha': 0.924280771708697, 'critic_loss': -7.285912782040699, 'actor_loss': 2.907983114698538, 'time_step': 0.09750191602345458, 'td_error': 0.1216746191484739, 'init_value': 0.8908321857452393, 'ave_value': 0.8893791525494973} step=1715\n",
      "2022-04-06 18:47.32 [info     ] Model parameters are saved to d3rlpy_logs/CQL_20220406184447/model_1715.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1,\n",
       "  {'time_sample_batch': 0.00034665783362207886,\n",
       "   'time_algorithm_update': 0.09301192836928299,\n",
       "   'temp_loss': 4.434645806387632,\n",
       "   'temp': 0.9835434387446145,\n",
       "   'alpha_loss': -10.887004488411172,\n",
       "   'alpha': 1.0149100758243927,\n",
       "   'critic_loss': 11.504611487291298,\n",
       "   'actor_loss': -0.4734545949420216,\n",
       "   'time_step': 0.09343911398951583,\n",
       "   'td_error': 0.23003137005684537,\n",
       "   'init_value': 0.358516663312912,\n",
       "   'ave_value': 0.3585621578675918}),\n",
       " (2,\n",
       "  {'time_sample_batch': 0.00030645267608909495,\n",
       "   'time_algorithm_update': 0.09020866944560504,\n",
       "   'temp_loss': 2.2606965619690564,\n",
       "   'temp': 0.9586732479643196,\n",
       "   'alpha_loss': 2.6586992984132585,\n",
       "   'alpha': 1.0215142763738383,\n",
       "   'critic_loss': -1.0089621576901429,\n",
       "   'actor_loss': 1.6619744485043229,\n",
       "   'time_step': 0.09059124368272788,\n",
       "   'td_error': 0.2244357153328237,\n",
       "   'init_value': 1.1041191816329956,\n",
       "   'ave_value': 1.1022191792748874}),\n",
       " (3,\n",
       "  {'time_sample_batch': 0.00031283228459928194,\n",
       "   'time_algorithm_update': 0.0978050558629606,\n",
       "   'temp_loss': 1.3170170980361737,\n",
       "   'temp': 0.9418875839202814,\n",
       "   'alpha_loss': 7.240036223442145,\n",
       "   'alpha': 0.9993701052387671,\n",
       "   'critic_loss': -4.852770173514897,\n",
       "   'actor_loss': 2.0584373998920005,\n",
       "   'time_step': 0.09819418626345977,\n",
       "   'td_error': 0.15648950364463302,\n",
       "   'init_value': 1.2321447134017944,\n",
       "   'ave_value': 1.228177076818056}),\n",
       " (4,\n",
       "  {'time_sample_batch': 0.0003068141269961877,\n",
       "   'time_algorithm_update': 0.09243900157272295,\n",
       "   'temp_loss': 0.7861016806291075,\n",
       "   'temp': 0.9292568977998228,\n",
       "   'alpha_loss': 9.583377957691605,\n",
       "   'alpha': 0.9631524921853758,\n",
       "   'critic_loss': -6.50581574926571,\n",
       "   'actor_loss': 2.4692244161322234,\n",
       "   'time_step': 0.09282089391880759,\n",
       "   'td_error': 0.11061606113975374,\n",
       "   'init_value': 1.2937169075012207,\n",
       "   'ave_value': 1.2894880254542218}),\n",
       " (5,\n",
       "  {'time_sample_batch': 0.0003132173688349154,\n",
       "   'time_algorithm_update': 0.09711221842307044,\n",
       "   'temp_loss': 0.4085727234102199,\n",
       "   'temp': 0.9204262795670735,\n",
       "   'alpha_loss': 10.972351727610775,\n",
       "   'alpha': 0.924280771708697,\n",
       "   'critic_loss': -7.285912782040699,\n",
       "   'actor_loss': 2.907983114698538,\n",
       "   'time_step': 0.09750191602345458,\n",
       "   'td_error': 0.1216746191484739,\n",
       "   'init_value': 0.8908321857452393,\n",
       "   'ave_value': 0.8893791525494973})]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_episodes,\n",
    "        eval_episodes=test_episodes,\n",
    "        n_epochs=5,\n",
    "        scorers={\n",
    "            'td_error': td_error_scorer,\n",
    "            'init_value': initial_state_value_estimation_scorer,\n",
    "            'ave_value': average_value_estimation_scorer\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbb0460-f723-4b67-a553-57887d11dbc9",
   "metadata": {},
   "source": [
    "## Off-Policy Evaluation\n",
    "\n",
    "We do get some metrics on a test set of initial state value and average value. However, these estimates (using the critic's Q-function) of model performance are biased. They're useful for validation during training, but not much else. Instead, we fit a Q-function to the data (or a separate dataset, as I've done here) separately and evaluate the model's performance on it.\n",
    "\n",
    "Feel free to change the chunks and number of steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5d639afd-476b-4844-9921-f001f81abf34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00000000e+00  7.95731469e+08 -7.24891077e-02 -1.35999953e-02\n",
      " -4.20001342e-03  0.00000000e+00 -5.33423489e+00 -1.57091618e+00\n",
      " -6.23311010e-02 -1.64283998e-01  6.00000000e-01]\n",
      "Read chunk # 5 out of 10000\n",
      "[ 0.00000000e+00  7.95731469e+08  7.01089229e-03 -4.19999531e-03\n",
      "  7.39998658e-03  0.00000000e+00 -5.33423489e+00 -1.57091618e+00\n",
      "  2.21623335e-01 -2.86362315e-02 -8.00043364e-02]\n",
      "Read chunk # 7 out of 10000\n",
      "[ 0.00000000e+00  7.95731469e+08 -1.03989108e-01 -1.37999953e-02\n",
      "  7.99998658e-03  0.00000000e+00 -5.33423489e+00 -1.57091618e+00\n",
      "  2.76352555e-01 -3.26280816e-01  6.00000000e-01]\n",
      "Read chunk # 9 out of 10000\n",
      "[ 0.00000000e+00  7.95731469e+08  9.46108923e-02 -1.43999953e-02\n",
      " -1.50001342e-03  0.00000000e+00 -5.33423489e+00 -1.57091618e+00\n",
      "  1.56237335e-02  2.60569314e-01 -6.00000000e-01]\n",
      "Read chunk # 11 out of 10000\n",
      "torch.Size([111080, 6])\n",
      "2022-04-06 18:50.25 [debug    ] RandomIterator is selected.\n",
      "2022-04-06 18:50.25 [info     ] Directory is created at d3rlpy_logs/FQE_20220406185025\n",
      "2022-04-06 18:50.25 [debug    ] Building models...\n",
      "2022-04-06 18:50.25 [debug    ] Models have been built.\n",
      "2022-04-06 18:50.25 [info     ] Parameters are saved to d3rlpy_logs/FQE_20220406185025/params.json params={'action_scaler': None, 'batch_size': 100, 'encoder_factory': {'type': 'default', 'params': {'activation': 'relu', 'use_batch_norm': False, 'dropout_rate': None}}, 'gamma': 0.99, 'generated_maxlen': 100000, 'learning_rate': 0.0001, 'n_critics': 1, 'n_frames': 1, 'n_steps': 1, 'optim_factory': {'optim_cls': 'Adam', 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False}, 'q_func_factory': {'type': 'mean', 'params': {'bootstrap': False, 'share_encoder': False}}, 'real_ratio': 1.0, 'reward_scaler': None, 'scaler': None, 'target_update_interval': 100, 'use_gpu': None, 'algorithm': 'FQE', 'observation_shape': (6,), 'action_size': 3}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea0fadec84f0412a93c3002a728e6f9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/1:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-06 18:50.45 [info     ] FQE_20220406185025: epoch=1 step=10000 epoch=1 metrics={'time_sample_batch': 0.00022144982814788818, 'time_algorithm_update': 0.0016423542022705078, 'loss': 0.008170671206247244, 'time_step': 0.0019714040517807008, 'init_value': -1.2672375440597534, 'ave_value': -1.2678414611500632, 'soft_opc': nan} step=10000\n",
      "2022-04-06 18:50.45 [info     ] Model parameters are saved to d3rlpy_logs/FQE_20220406185025/model_10000.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1,\n",
       "  {'time_sample_batch': 0.00022144982814788818,\n",
       "   'time_algorithm_update': 0.0016423542022705078,\n",
       "   'loss': 0.008170671206247244,\n",
       "   'time_step': 0.0019714040517807008,\n",
       "   'init_value': -1.2672375440597534,\n",
       "   'ave_value': -1.2678414611500632,\n",
       "   'soft_opc': nan})]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from d3rlpy.ope import FQE\n",
    "# metrics to evaluate with\n",
    "from d3rlpy.metrics.scorer import soft_opc_scorer\n",
    "\n",
    "\n",
    "ope_dataset = get_dataset([4,6,8,10]) #change if you'd prefer different chunks\n",
    "ope_train_episodes, ope_test_episodes = train_test_split(ope_dataset, test_size=0.2)\n",
    "\n",
    "fqe = FQE(algo=model, use_gpu=False) #change this if you have one!\n",
    "fqe.fit(ope_train_episodes, eval_episodes=ope_test_episodes,\n",
    "        n_steps=10000, #change if overfitting/underfitting\n",
    "        scorers={\n",
    "           'init_value': initial_state_value_estimation_scorer,\n",
    "            'ave_value': average_value_estimation_scorer,\n",
    "           'soft_opc': soft_opc_scorer(return_threshold=0)\n",
    "        })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
